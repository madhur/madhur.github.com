<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Coding it my way</title>
  <link href="https://madhur.co.in/blog/"/>
 <updated>2021-11-13T18:56:20+05:30</updated>
 <id>https://madhur.co.in/blog/</id>
 <author>
   <name>Madhur Ahuja</name>
   <email>ahuja.madhur@gmail.com</email>
 </author>
 
 
 <entry>
   <title>Conventional Commits</title>
   <link href="https://madhur.co.in/blog/2021/11/13/conventional-commits.html"/>
   <updated>2021-11-13T00:00:00+05:30</updated>
   <id>id:/blog/2021/11/13/conventional-commits</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://www.conventionalcommits.org/en/v1.0.0/#specification&quot;&gt;Conventional Commits&lt;/a&gt; is a an effort to standardizing writing better git commit messages.&lt;/p&gt;

&lt;p&gt;As per it, a commit message should be structured as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;type&amp;gt;[optional scope]: &amp;lt;description&amp;gt;

[optional body]

[optional footer(s)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We recently standardized writing commit messages in our team according to this spec. This can be even forced through &lt;a href=&quot;https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks&quot;&gt;git commit hooks&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;commit_regex_normal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;(((feat|docs|style|refactor|perf|test|build|ci|chore|revert)(&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\w&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;{0,15})&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;)?))(&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.*&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\S&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.*)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;commit_regex_bug_fix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;(((Fix)(&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\w&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;{0,15})&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;)?))(&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;)([A-Z]+-[0-9]+:)(.*&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\S&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.*)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;commit_regex_auto_gen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;(Merge.*)|(Revert.*)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;|&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;commit_regex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$commit_regex_normal$o$commit_regex_bug_fix$o$commit_regex_auto_gen&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;error_msg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;       /‾‾‾‾‾‾‾‾
    &amp;lt;  Please use semantic commit messages(see https://www.conventionalcommits.org/en/v1.0.0 )
       \________

  &amp;lt;type&amp;gt;[&amp;lt;scope&amp;gt;]: &amp;lt;short summary&amp;gt;
     │     |              │
     │   (optional)       └─&amp;gt; Summary in present tense. Not capitalized. No period at the end.
     │
     └─&amp;gt; Type: chore, docs, feat, fix, refactor, style, or test.
    fix[&amp;lt;scope&amp;gt;]: ABC-1234 summary (Jira id mandatory for type -fix)

&apos;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-iqE&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;commit_regex&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then
    &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;error_msg&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&amp;amp;2
    &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;1
&lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Querying AWS ALB Logs using Athena</title>
   <link href="https://madhur.co.in/blog/2021/11/06/querying-alb-logs-aws-athena.html"/>
   <updated>2021-11-06T00:00:00+05:30</updated>
   <id>id:/blog/2021/11/06/querying-alb-logs-aws-athena</id>
   <content type="html">&lt;p&gt;Recently, I had a requirement of querying &lt;a href=&quot;https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html#:~:text=Elastic%20Load%20Balancing%20automatically%20distributes,only%20to%20the%20healthy%20targets.&quot;&gt;AWS Application Load Balancer&lt;/a&gt; Logs to get some data around request/ sec and p95 latencies.&lt;/p&gt;

&lt;p&gt;The Application load balancer logs are stored in &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;AWS S3&lt;/a&gt; by default and follows a consistent format which is &lt;a href=&quot;https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html&quot;&gt;documented here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;amp;whats-new-cards.sort-order=desc&quot;&gt;AWS Athena&lt;/a&gt; is the best tool to query such logs.&lt;/p&gt;

&lt;h2 id=&quot;best-practices-using-aws-athena&quot;&gt;Best practices using AWS Athena&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Make sure you specify the time period when querying Athena, else the data scanned will be very huge and you will end up paying lot more.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To find out the relevant time period to query, have a look at the &lt;a href=&quot;https://aws.amazon.com/cloudwatch/&quot;&gt;AWS Cloudwatch&lt;/a&gt; metrics and find intreseting patterns such as spikes in request count, response time etc&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If your ALB has comples routing logic, make sure to specify the &lt;a href=&quot;https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html&quot;&gt;Target group&lt;/a&gt; in the query&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;find-url-and-times-it-was-called-within-the-specified-time-period&quot;&gt;Find url and times it was called within the specified time period&lt;/h3&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;&quot;alb_logs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;&amp;lt;alb_name&amp;gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;2021&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;month&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;10&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;day&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;24&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;request_creation_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;2021-10-24T13:37:00.000000Z&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request_creation_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;2021-10-24T13:38:00.000000Z&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request_url&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;order&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;desc&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;limit&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;find-p95-latency-by-url&quot;&gt;Find p95 Latency by url&lt;/h3&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;approx_percentile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_processing_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p95&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;&quot;alb_logs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;&amp;lt;alb_name&amp;gt;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;2021&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;month&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;10&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;day&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;24&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request_creation_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;2021-10-24T13:43:00.000000Z&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request_creation_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;2021-10-24T13:44:00.000000Z&apos;&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request_url&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;order&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p95&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;desc&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;limit&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Backup and Restore specific cassandra tables</title>
   <link href="https://madhur.co.in/blog/2021/10/30/backup-and-restore-specific-cassandra-tables.html"/>
   <updated>2021-10-30T00:00:00+05:30</updated>
   <id>id:/blog/2021/10/30/backup-and-restore-specific-cassandra-tables</id>
   <content type="html">&lt;p&gt;Recently, we had to make alterations directly to production cassandra tables due to an urgent bug fix and required to backup and restore specific cassandra tables so that we could roll back incase something gone wrong.&lt;/p&gt;

&lt;p&gt;Backup and restoring tables is seamless using &lt;a href=&quot;https://cassandra.apache.org/doc/latest/cassandra/tools/nodetool/nodetool.html&quot;&gt;nodetool&lt;/a&gt; and &lt;a href=&quot;https://cassandra.apache.org/doc/latest/cassandra/tools/sstable/sstableloader.html&quot;&gt;sstableloader&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;cassandra-table-backup-steps&quot;&gt;Cassandra table backup steps&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Login to Casandra Box&lt;/li&gt;
  &lt;li&gt;Execute the following commands from the cassandra bin directory&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   ./nodetool snapshot -cf &amp;lt;tablename&amp;gt; &amp;lt;keyspace&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Repeat the same for any other table to be backed up&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This will create snapshots in the data directory at following locations:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  cassandra/data/data/&amp;lt;keyspace&amp;gt;/&amp;lt;table&amp;gt;-&amp;lt;UID&amp;gt;/snapshots/&amp;lt;epoch&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;cassandra-table-restore-steps&quot;&gt;Cassandra table restore steps&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Login to cassandra box&lt;/li&gt;
  &lt;li&gt;Navigate to following folder&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  cassandra/data/data/f&amp;lt;keyspace&amp;gt;/&amp;lt;table&amp;gt;-&amp;lt;UID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Execute following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  cp snapshots/&amp;lt;epoch&amp;gt;/*.* .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Repeat the same for any other table to be restored&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once all the snapshots have been copied to data directory, run following commands in sequence:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  ./nodetool refresh &amp;lt;keyspace&amp;gt; &amp;lt;table&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Using sshpass to skip entering passwords for SSH</title>
   <link href="https://madhur.co.in/blog/2021/10/23/using-sshpass-to-skip-entering-password.html"/>
   <updated>2021-10-23T00:00:00+05:30</updated>
   <id>id:/blog/2021/10/23/using-sshpass-to-skip-entering-password</id>
   <content type="html">&lt;p&gt;If your server does not allow key based login, you might need to enter password each time you want to ssh into.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh&lt;/code&gt; does not allow entering password into the command line.&lt;/p&gt;

&lt;p&gt;There is a nifty tool to allow exactly that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sshpass&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install sshpass
$ sshpass -p your_password ssh user@hostname
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On Mac&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew install hudochenkov/sshpass/sshpass
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Running Spark on Windows</title>
   <link href="https://madhur.co.in/blog/2021/10/16/running-spark-on-windows.html"/>
   <updated>2021-10-16T00:00:00+05:30</updated>
   <id>id:/blog/2021/10/16/running-spark-on-windows</id>
   <content type="html">&lt;p&gt;Assuming you have spark downloaded on machine, you would run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./spark-class org.apache.spark.deploy.master.Master&lt;/code&gt; to run the Spark Master controller&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PS D:\spark\spark\bin&amp;gt;  ./spark-class org.apache.spark.deploy.master.Master
Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.properties
21/10/16 09:53:02 INFO Master: Started daemon with process name: 4804@DESKTOP-QCP2G4K
21/10/16 09:53:02 WARN Shell: Did not find winutils.exe: {}
21/10/16 09:53:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/10/16 09:53:02 INFO SecurityManager: Changing view acls to: user
21/10/16 09:53:02 INFO SecurityManager: Changing modify acls to: user
21/10/16 09:53:02 INFO SecurityManager: Changing view acls groups to:
21/10/16 09:53:02 INFO SecurityManager: Changing modify acls groups to:
21/10/16 09:53:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(user); groups with view permissions: Set(); users  with modify permissions: Set(user); groups with modify permissions: Set()
21/10/16 09:53:03 INFO Utils: Successfully started service &apos;sparkMaster&apos; on port 7077.
21/10/16 09:53:03 INFO Master: Starting Spark master at spark://172.21.128.1:7077
21/10/16 09:53:03 INFO Master: Running Spark version 3.1.2
21/10/16 09:53:03 INFO Utils: Successfully started service &apos;MasterUI&apos; on port 8080.
21/10/16 09:53:03 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://DESKTOP-QCP2G4K.mshome.net:8080
21/10/16 09:53:03 INFO Master: I have been elected leader! New state: ALIVE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The spark UI will now be running at http://localhost:8080&lt;/p&gt;

&lt;p&gt;Now, we want to run the spark executor using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./spark-class org.apache.spark.deploy.worker.Worker spark://172.21.128.1:7077&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PS D:\spark\spark\bin&amp;gt; ./spark-class org.apache.spark.deploy.worker.Worker spark://172.21.128.1:7077
Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.properties
21/10/16 09:56:12 INFO Worker: Started daemon with process name: 8800@DESKTOP-QCP2G4K
21/10/16 09:56:12 WARN Shell: Did not find winutils.exe: {}
21/10/16 09:56:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/10/16 09:56:12 INFO SecurityManager: Changing view acls to: user
21/10/16 09:56:12 INFO SecurityManager: Changing modify acls to: user
21/10/16 09:56:12 INFO SecurityManager: Changing view acls groups to:
21/10/16 09:56:12 INFO SecurityManager: Changing modify acls groups to:
21/10/16 09:56:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(user); groups with view permissions: Set(); users  with modify permissions: Set(user); groups with modify permissions: Set()
21/10/16 09:56:12 INFO Utils: Successfully started service &apos;sparkWorker&apos; on port 57088.
21/10/16 09:56:12 INFO Worker: Worker decommissioning not enabled, SIGPWR will result in exiting.
21/10/16 09:56:13 INFO Worker: Starting Spark worker 172.21.128.1:57088 with 12 cores, 30.9 GiB RAM
21/10/16 09:56:13 INFO Worker: Running Spark version 3.1.2
21/10/16 09:56:13 INFO Worker: Spark home: D:\spark\spark\bin\..
21/10/16 09:56:13 INFO ResourceUtils: ==============================================================
21/10/16 09:56:13 INFO ResourceUtils: No custom resources configured for spark.worker.
21/10/16 09:56:13 INFO ResourceUtils: ==============================================================
21/10/16 09:56:13 INFO Utils: Successfully started service &apos;WorkerUI&apos; on port 8081.
21/10/16 09:56:13 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://DESKTOP-QCP2G4K.mshome.net:8081
21/10/16 09:56:13 INFO Worker: Connecting to master 172.21.128.1:7077...
21/10/16 09:56:13 INFO TransportClientFactory: Successfully created connection to /172.21.128.1:7077 after 21 ms (0 ms spent in bootstraps)
21/10/16 09:56:13 INFO Worker: Successfully registered with master spark://172.21.128.1:7077
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Spark is now ready to be used in any application such as Java.&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nd&quot;&gt;@Configuration&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SparkConfig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;nd&quot;&gt;@Value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;${spark.app.name}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;appName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;nd&quot;&gt;@Value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;${spark.master}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;masterUri&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;nd&quot;&gt;@Bean&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SparkConf&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;SparkConf&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;SparkConf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
                        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setAppName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;local-1634217252353&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
                        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;setMaster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;spark://172.21.128.1:7077&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;nd&quot;&gt;@Bean&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;JavaSparkContext&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;JavaSparkContext&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;JavaSparkContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;());&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>zsh Autocomplete</title>
   <link href="https://madhur.co.in/blog/2021/10/09/zsh-autocomplete.html"/>
   <updated>2021-10-09T00:00:00+05:30</updated>
   <id>id:/blog/2021/10/09/zsh-autocomplete</id>
   <content type="html">&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zsh&lt;/code&gt; shell by default does not come with auto suggestions facility unlike fish shell.&lt;/p&gt;

&lt;p&gt;If you are looking for auto suggestions similar to fish, have a look at https://github.com/zsh-users/zsh-autosuggestions&lt;/p&gt;

&lt;p&gt;For MAC OS, following steps are required to install&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.zshrc&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plugins=( 
    # other plugins...
    zsh-autosuggestions
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Designing Change Data Capture</title>
   <link href="https://madhur.co.in/blog/2021/08/20/design-change-data-capture.html"/>
   <updated>2021-08-20T00:00:00+05:30</updated>
   <id>id:/blog/2021/08/20/design-change-data-capture</id>
   <content type="html">&lt;p&gt;Change Data Capture (CDC) is a very important topic for any backend developer to understand. Lot of systems backbone is based on Change data capture.&lt;/p&gt;

&lt;p&gt;When I started my career as a backend developer, I did not have any understand of CDC. It is only after talking to some of the industry experts, I realized how important this topic is.&lt;/p&gt;

&lt;p&gt;In this post, we will try to understand what is CDC and its practical usages:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Change Data Capture is a software process that identifies and tracks changes to data in a source database. The source database can be relational or any database for that matter. CDC provides real time or near real time movement of data by moving and processing data continuously as new database events occur.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One of the example of CDC is Microsoft’s Cosmos DB Change Feed&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/cosmos-db/change-feed&quot;&gt;Change feed in Azure Cosmos DB&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://debezium.io/&quot;&gt;Debezium&lt;/a&gt; is another solution which provides CDC for the databases such as MySQL&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;change-data-capture-methods&quot;&gt;Change data capture Methods&lt;/h2&gt;

&lt;h3 id=&quot;audit-columns&quot;&gt;Audit Columns&lt;/h3&gt;

&lt;p&gt;By using columns such as LAST_UPDATED, DATE_MODIFIED columns. The application can query the rows which have modified that changed since data was last extracted and publish into the stream.&lt;/p&gt;

&lt;h3 id=&quot;trigger-based-cdc&quot;&gt;Trigger Based CDC&lt;/h3&gt;

&lt;p&gt;Defining database triggers that fire after INSERT, UPDATE OR DELETE commands are another method use to create a change log. Some databases have native support for triggers.&lt;/p&gt;

&lt;h3 id=&quot;log-based-cdc&quot;&gt;Log based CDC&lt;/h3&gt;

&lt;p&gt;Databases support transaction logs that store all database events. With log based CDC, new database transactions are read from source database native transaction logs.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The changes are captured without making application level changes and without having to &amp;gt; scan operational tables, both of which add additional workload and reduce source &amp;gt; systems’ performance.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;using-cdc-to-implement-the-outbox-pattern&quot;&gt;Using CDC to implement the Outbox Pattern&lt;/h2&gt;

&lt;p&gt;Content taken from https://debezium.io/blog/2020/02/10/event-sourcing-vs-cdc/&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The primary goal of the Outbox Pattern is to ensure that updates to the application  state (stored in tables) and publishing of the respective domain event is done within a  single transaction. This involves creating an Outbox table in the database to collect  those domain events as part of a transaction. Having transactional guarantees around the  domain events and their propagation via the Outbox is important for data consistency  across a system.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;After the transaction completes, the domain events are then picked up by a CDC  connector and forwarded to interested consumers using a reliable message broker (see  Figure 5). Those consumers may then use the domain events to materialize their own  aggregates (see above per Event Sourcing)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/Blog/cdc.png&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Liveness and Readiness Probes</title>
   <link href="https://madhur.co.in/blog/2021/08/14/liveness-and-readiness-probes.html"/>
   <updated>2021-08-14T00:00:00+05:30</updated>
   <id>id:/blog/2021/08/14/liveness-and-readiness-probes</id>
   <content type="html">&lt;p&gt;When working with Kubernetes, &lt;a href=&quot;https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/&quot;&gt;Liveness and Readiness Probes&lt;/a&gt; are very important concepts to understand.&lt;/p&gt;

&lt;h2 id=&quot;readiness-probe&quot;&gt;Readiness Probe&lt;/h2&gt;

&lt;p&gt;Kubernetes fires readiness probe to the pod to determine if the pod is ready to serve traffic or not.&lt;/p&gt;

&lt;p&gt;It can simply be defined as&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;readinessProbe&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cat&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/tmp/healthy&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;initialDelaySeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;periodSeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can also provide readiness probe as HTTP or TCP commands&lt;/p&gt;

&lt;h2 id=&quot;liveness-probe&quot;&gt;Liveness Probe&lt;/h2&gt;

&lt;p&gt;Kubernetes fires readiness probe to the pod to determine if the pod is alive. If the pod is not alive, Kubernetes will restart the pod.&lt;/p&gt;

&lt;p&gt;An important point to be noted is&lt;/p&gt;

&lt;p&gt;` Liveness probes do not wait for readiness probes to succeed. If you want to wait before executing a liveness probe you should use initialDelaySeconds or a startupProbe.`&lt;/p&gt;

&lt;p&gt;Which means that liveness probe and readiness probes have no dependency on each other. Some people assume that liveness probe start only after readiness probe is successful. This is misconception and is incorrect.&lt;/p&gt;

&lt;h3 id=&quot;how-to-determine-the-value-of-these-probes&quot;&gt;How to determine the value of these probes&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Determine the max start-up time taken by the application server to successfully start accepting http connections.
For example, in our case pod takes around 15 seconds in all different environments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Assuming a buffer of about 15 seconds we set the liveness probe to 30 seconds, with the default retry of 3 times and time difference between each retry as 5 seconds (these default value are set in periodSeconds and failureThreshold for each probe).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This leads to liveness probe being successful in the worst case scenario up to 45 seconds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With another buffer of 15 seconds we set the readiness probe for 60 seconds.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;startup-probes&quot;&gt;Startup Probes&lt;/h2&gt;

&lt;p&gt;Kubernetes introduced a new type of probe called startup probe to introduce the delay for liveness probes.&lt;/p&gt;

&lt;p&gt;As per kubernetes&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sometimes, you have to deal with legacy applications that might require an additional startup time on their first initialization. In &amp;gt; such cases, it can be tricky to set up liveness probe parameters without compromising the fast response to deadlocks that motivated such &amp;gt; a probe. The trick is to set up a startup probe with the same command, HTTP or TCP check, with a failureThreshold * periodSeconds long &amp;gt; enough to cover the worse case startup time.&lt;/p&gt;
&lt;/blockquote&gt;
</content>
 </entry>
 
 <entry>
   <title>Meaning of at-least once, at-most once and exactly-once delivery</title>
   <link href="https://madhur.co.in/blog/2021/07/18/meaning-of-at-least-once-at-most-once-and-exactly-once-delivery.html"/>
   <updated>2021-07-18T00:00:00+05:30</updated>
   <id>id:/blog/2021/07/18/meaning-of-at-least-once-at-most-once-and-exactly-once-delivery</id>
   <content type="html">&lt;p&gt;Ever since I have started working with Kafka, I have came across these terms very frequently, At-least once, At-most once and Exactly Once.&lt;/p&gt;

&lt;p&gt;As an engineer, It is very important to understand these concepts.&lt;/p&gt;
&lt;h2 id=&quot;at-most-once-configuration&quot;&gt;At-most once Configuration&lt;/h2&gt;

&lt;p&gt;As the name suggests, At-most-once means the message will be delivered at-most once. Once delivered, there is no chance of delivering again. If the consumer is unable to handle the message due to some exception, the message is lost. This is because Kafka is automatically committing the last offset used.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enable.auto.commit&lt;/code&gt; to true&lt;/li&gt;
  &lt;li&gt;Set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto.commit.interval.ms&lt;/code&gt; to low value&lt;/li&gt;
  &lt;li&gt;Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto.commit&lt;/code&gt; is set to true, there is no need to call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;consumer.commitSync()&lt;/code&gt; from the consumer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that it is also possible to have at-lest-once scenario with the same configuration. Let’s say consumer successfully processed the message successfully into its store and in the meantime before kafka could commit the offset, consumer was restarted. In this scenario, consumer would again get the same message.&lt;/p&gt;

&lt;p&gt;Hence, even if using at-most once or at-least once configuration, consumer should be always prepared to handle the duplicates.&lt;/p&gt;

&lt;h2 id=&quot;at-least-once-configuration&quot;&gt;At-least once configuration&lt;/h2&gt;
&lt;p&gt;At-least once as the name suggests, message will be delivered atleast once. There is high chance that message will be delivered again as duplicate.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enable.auto.commit&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt; OR&lt;/li&gt;
  &lt;li&gt;Consumer should now then take control of the message offset commits to Kafka by making the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;consumer.commitSync()&lt;/code&gt; call.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s say consumer has processed the messages and committed the messages to its local store, but consumer crashes and did not get a chance to commit offset to Kafka before it has crashed. When consumer restarts, Kafka would deliver messages from the last offset, resulting in duplicates.&lt;/p&gt;

&lt;h2 id=&quot;exactly-once-configuration&quot;&gt;Exactly-once configuration&lt;/h2&gt;

&lt;p&gt;Exactly-once as the name suggests, there will be only one and once message delivery. It difficult to achieve in practice.&lt;/p&gt;

&lt;p&gt;In this case offset needs to be manually managed.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enable.auto.commit&lt;/code&gt; to false&lt;/li&gt;
  &lt;li&gt;Do not make call to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;consumer.commitSync()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Implement a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ConsumerRebalanceListener&lt;/code&gt; and within the listener perform &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;consumer.seek(topicPartition,offset);&lt;/code&gt; to start reading from a specific offset of that topic/partition.&lt;/li&gt;
  &lt;li&gt;While processing the messages, get hold of the offset of each message. Store the processed message’s offset in an atomic way along with the processed message using atomic-transaction. When data is stored in relational database atomicity is easier to implement. For non-relational data-store such as HDFS store or No-SQL store one way to achieve atomicity is as follows: Store the offset along with the message.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>How tinyUrl Does redirection</title>
   <link href="https://madhur.co.in/blog/2021/07/11/how-tinyurl-does-redirection.html"/>
   <updated>2021-07-11T00:00:00+05:30</updated>
   <id>id:/blog/2021/07/11/how-tinyurl-does-redirection</id>
   <content type="html">&lt;p&gt;I got curious about how tinyUrl does redirection for its Url. Is it standard browser redirect with 301/302 status code or something else?&lt;/p&gt;

&lt;p&gt;With that created this tinyUrl &lt;a href=&quot;https://tinyurl.com/madhur25&quot;&gt;tinyurl.com/madhur25&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Figured out that it was indeed a browser redirect using 301 status code.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ curl https://tinyurl.com/madhur25 -vvv
* STATE: INIT =&amp;gt; CONNECT handle 0x600057310; line 1407 (connection #-5000)
* Added connection 0. The cache now contains 1 members
*   Trying 104.20.138.65...
* TCP_NODELAY set
* STATE: CONNECT =&amp;gt; WAITCONNECT handle 0x600057310; line 1460 (connection #0)
* Connected to tinyurl.com (104.20.138.65) port 443 (#0)
* STATE: WAITCONNECT =&amp;gt; SENDPROTOCONNECT handle 0x600057310; line 1567 (connection #0)
* Marked for [keep alive]: HTTP default
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /usr/ssl/certs/ca-bundle.crt
  CApath: none
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* STATE: SENDPROTOCONNECT =&amp;gt; PROTOCONNECT handle 0x600057310; line 1581 (connection #0)
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-ECDSA-AES128-GCM-SHA256
* ALPN, server accepted to use http/1.1
* Server certificate:
*  subject: C=US; ST=California; L=San Francisco; O=Cloudflare, Inc.; CN=sni.cloudflaressl.com
*  start date: Jul  3 00:00:00 2021 GMT
*  expire date: Jul  2 23:59:59 2022 GMT
*  subjectAltName: host &quot;tinyurl.com&quot; matched cert&apos;s &quot;tinyurl.com&quot;
*  issuer: C=US; O=Cloudflare, Inc.; CN=Cloudflare Inc ECC CA-3
*  SSL certificate verify ok.
* STATE: PROTOCONNECT =&amp;gt; DO handle 0x600057310; line 1602 (connection #0)
&amp;gt; GET /madhur25 HTTP/1.1
&amp;gt; Host: tinyurl.com
&amp;gt; User-Agent: curl/7.51.0
&amp;gt; Accept: */*
&amp;gt;
* STATE: DO =&amp;gt; DO_DONE handle 0x600057310; line 1664 (connection #0)
* STATE: DO_DONE =&amp;gt; WAITPERFORM handle 0x600057310; line 1791 (connection #0)
* STATE: WAITPERFORM =&amp;gt; PERFORM handle 0x600057310; line 1801 (connection #0)
* HTTP 1.1 or later with persistent connection, pipelining supported
&amp;lt; HTTP/1.1 301 Moved Permanently
&amp;lt; Date: Sun, 11 Jul 2021 04:11:01 GMT
&amp;lt; Content-Type: text/html; charset=UTF-8
&amp;lt; Transfer-Encoding: chunked
&amp;lt; Connection: keep-alive
&amp;lt; X-Powered-By: PHP/7.3.28
&amp;lt; Location: https://www.madhur.co.in/
&amp;lt; Cache-Control: max-age=0, public, s-max-age=900, stale-if-error: 86400
&amp;lt; Referrer-Policy: unsafe-url
&amp;lt; Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
&amp;lt; CF-Cache-Status: DYNAMIC
&amp;lt; Expect-CT: max-age=604800, report-uri=&quot;https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct&quot;
* Server cloudflare is not blacklisted
&amp;lt; Server: cloudflare
&amp;lt; CF-RAY: 66cf2f6e7e293c13-BLR
&amp;lt; alt-svc: h3-27=&quot;:443&quot;; ma=86400, h3-28=&quot;:443&quot;; ma=86400, h3-29=&quot;:443&quot;; ma=86400, h3=&quot;:443&quot;; ma=86400
&amp;lt;
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;
        &amp;lt;meta charset=&quot;UTF-8&quot; /&amp;gt;
        &amp;lt;meta http-equiv=&quot;refresh&quot; content=&quot;0;url=&apos;https://www.madhur.co.in/&apos;&quot; /&amp;gt;

        &amp;lt;title&amp;gt;Redirecting to https://www.madhur.co.in/&amp;lt;/title&amp;gt;
    &amp;lt;/head&amp;gt;
    &amp;lt;body&amp;gt;
        Redirecting to &amp;lt;a href=&quot;https://www.madhur.co.in/&quot;&amp;gt;https://www.madhur.co.in/&amp;lt;/a&amp;gt;.
    &amp;lt;/body&amp;gt;
* STATE: PERFORM =&amp;gt; DONE handle 0x600057310; line 1965 (connection #0)
* multi_done
* Curl_http_done: called premature == 0
* Connection #0 to host tinyurl.com left intact
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Encoding and Hashing</title>
   <link href="https://madhur.co.in/blog/2021/07/06/encoding-and-hashing.html"/>
   <updated>2021-07-06T00:00:00+05:30</updated>
   <id>id:/blog/2021/07/06/encoding-and-hashing</id>
   <content type="html">&lt;h2 id=&quot;md5&quot;&gt;MD5&lt;/h2&gt;

&lt;p&gt;MD5 is hashing algorithm. It produces 128 bit value&lt;/p&gt;

&lt;h2 id=&quot;base64-encoding&quot;&gt;Base64 Encoding&lt;/h2&gt;

&lt;p&gt;Base64 character encodes 6 bits of the hash value. So if we use base64 encoding of above value, it will contain 21 characters&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Capacity Estimation Examples</title>
   <link href="https://madhur.co.in/blog/2021/06/27/capacity-estimation.html"/>
   <updated>2021-06-27T00:00:00+05:30</updated>
   <id>id:/blog/2021/06/27/capacity-estimation</id>
   <content type="html">&lt;p&gt;Capacity estimation is one of the most important exercises in design.&lt;/p&gt;

&lt;p&gt;Twitter Example&lt;/p&gt;

&lt;p&gt;1B Total Users
200 M DAU
100 M new tweets everyday
Each user follows 200 people avg&lt;/p&gt;

&lt;p&gt;How many favorites per day?
Each user 5 favorite per day
200M *5 = 1B favorite&lt;/p&gt;

&lt;p&gt;Total tweet views
A User visits their timeline 2 times a day and visits 5 other people pages
Each page has 20 tweets&lt;/p&gt;

&lt;p&gt;200M &lt;em&gt;(2+5)&lt;/em&gt;20 = 28B / day&lt;/p&gt;

&lt;p&gt;Storage&lt;/p&gt;

&lt;p&gt;140 characters ~ 280 bytes
100M*(280+30) bytes =&amp;gt; 30 GB / day&lt;/p&gt;

&lt;p&gt;Not all tweets will have media, let’s assume that on average every fifth tweet has a photo and every tenth has a video. Let’s also assume on average a photo is 200KB and a video is 2MB. This will lead us to have 24TB of new media every day.
(100M/5 photos * 200KB) + (100M/10 videos * 2MB) ~= 24TB/day&lt;/p&gt;

&lt;p&gt;Bandwidth Estimates Since total ingress is 24TB per day, this would translate into 290MB/sec.&lt;/p&gt;

&lt;p&gt;Remember that we have 28B tweet views per day. We must show the photo of every tweet (if it has a photo), but let’s assume that the users watch every 3rd video they see in their timeline. So, total egress will be:
(28B * 280 bytes) / 86400s of text =&amp;gt; 93MB/s&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(28B/5 * 200KB ) / 86400s of photos =&amp;gt; 13GB/S&lt;/li&gt;
  &lt;li&gt;(28B/10/3 * 2MB ) / 86400s of Videos =&amp;gt; 22GB/s&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Total ~= 35GB/s&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Simple Design of Communication Service</title>
   <link href="https://madhur.co.in/blog/2021/06/20/simple-design-of-communication-service.html"/>
   <updated>2021-06-20T00:00:00+05:30</updated>
   <id>id:/blog/2021/06/20/simple-design-of-communication-service</id>
   <content type="html">&lt;p&gt;Few months back, I had to make some changes to design of Communication Service.&lt;/p&gt;

&lt;p&gt;It was quite a complex architecture overall and I thought of documenting this in blogpost to outline my thought process.&lt;/p&gt;

&lt;p&gt;A communication service is responsible for reliably delivering the customer notification to the end user via any channel.&lt;/p&gt;

&lt;p&gt;In our case, the channels were email, SMS and push notification.&lt;/p&gt;

&lt;p&gt;The communication service was divided into multiple microservices:&lt;/p&gt;

&lt;h2 id=&quot;communication-services&quot;&gt;Communication Services&lt;/h2&gt;
&lt;p&gt;This was the main service which recieved payloads from other microservices whenever they had to send out a communication.&lt;/p&gt;

&lt;p&gt;The payload was json and interaction was via kafka queue.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key points:&lt;/strong&gt;
We had to make sure we monitor the kafka queue for lag very closely. As any lag can potentially delay the communication to the end user.&lt;/p&gt;

&lt;p&gt;The communication service did the enrichment to the payload such as creating the email template, SMS template from its template database and pushed the enriched payload
to individiual channel kafka queues. We wanted to have individual channel queues for independently scaling them. For instance, our traffic on email was much higher compared to SMS and push notification. That way, we could scale the partitions on email kafka and queue.&lt;/p&gt;

&lt;p&gt;The communication service also generates a unique uuid for each request and persists into cassandra database. This serves as a single point of lookup for&lt;/p&gt;

&lt;h2 id=&quot;email--sms--push-service&quot;&gt;Email / SMS / Push Service&lt;/h2&gt;
&lt;p&gt;These are individual micro services listening to each of the channel queues. They make a rest call to the provider to actually deliver the email. Once the rest call is successfully made, we push the status message on a feedback kafka queue dedicated for each channel.The feedback kafka queues are again channel independent to allow them to independently scale.&lt;/p&gt;

&lt;p&gt;These services also contain webhook listeners. Typically, when you delivery a email / SMS / push via a vendor, vendor will send the feedback via webhook weather the mail / SMS / push was successfully delivered, or if it bounced or if it went to spam etc.&lt;/p&gt;

&lt;p&gt;It is important to have circuit breaker in these services as vendor services might go down any moment. Thus, it is important to have a dead letter queue, which would retry the failed pushes later.&lt;/p&gt;

&lt;p&gt;These feedbacks are also sent to the same feedback queues as updates.&lt;/p&gt;

&lt;p&gt;We could have had a separate service for feedback altogether, but I believe it didn’t require a separate microservice altogether which would expanded our monitoring footprint.&lt;/p&gt;

&lt;h2 id=&quot;feedback-service&quot;&gt;Feedback Service&lt;/h2&gt;

&lt;p&gt;This is the main services which receives all the updates w.r.t. each communication. These updates are consumed via feedback queues. It is important to scale this service sufficiently as we didn’t had a separate feedback service for each channel in order to have lesser complexity. Our email feedback kakfa topic had 200 partitions and there were 20 computes each running 10 threads to consume from these partitions.&lt;/p&gt;

&lt;p&gt;The job of the feedback service is to receive the feedback and push it to cassandra database. It asynchronously also pushes the feedback to analytics channel mainly an elasticsearch database for analytics purpose, since cassandra is not suited for doing analytics.&lt;/p&gt;

&lt;p&gt;The final design looked like this:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/Blog/comm_service.png&quot;&gt;&lt;img src=&quot;/images/Blog/comm_service.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>API Response Group Pattern - Improving latencies using GraphQL</title>
   <link href="https://madhur.co.in/blog/2021/06/13/api-response-group-pattern.html"/>
   <updated>2021-06-13T00:00:00+05:30</updated>
   <id>id:/blog/2021/06/13/api-response-group-pattern</id>
   <content type="html">&lt;p&gt;In a complex multiple microservices architecture, especially where in one microservice ends up calling a chain of other microservices, the API latency can become a concern.&lt;/p&gt;

&lt;p&gt;This is because there is network hop at each microservice layer, which adds to the latency in addition to the actual logic executed by the microservice itself.&lt;/p&gt;

&lt;p&gt;For example, in an e-commerce application, an Add to Cart API request might end up calling multiple downstream microservices such as Pricing Service, Availability Service etc.&lt;/p&gt;

&lt;p&gt;In a complex architecture, where multiple clients are involved such as Mobile Apps and web browsers, the data needed by clients can differ depending upon the situation.&lt;/p&gt;

&lt;p&gt;For example, if the user is adding an item to cart from the homepage, the client might not need pricing details as it might have that data already cached.&lt;/p&gt;

&lt;p&gt;In other scenario, if the user is just increasing the quantity of the item, there might not be need to call the availability service as the front end might have called it when user initially called it 
and front end could cache the availability data of the item for some time (lets say 15 minutes).&lt;/p&gt;

&lt;p&gt;To achieve the best API performance, it is important to recognize these patterns and incorporate into the API design.&lt;/p&gt;

&lt;p&gt;So instead of creating a separate API for each of these use cases, its better incorporate this by using
the &lt;em&gt;Response Group Pattern&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For example, in our hypothetical case, we might come up with following response groups:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Response Group&lt;/th&gt;
      &lt;th&gt;Pricing Service&lt;/th&gt;
      &lt;th&gt;Seller Service&lt;/th&gt;
      &lt;th&gt;Asset Service&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;full&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;basic&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;x&lt;/td&gt;
      &lt;td&gt;x&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;summary&lt;/td&gt;
      &lt;td&gt;x&lt;/td&gt;
      &lt;td&gt;✓&lt;/td&gt;
      &lt;td&gt;x&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
As shown above, we don’t want to call Seller and Asset service when addToCart is invoked with response group basic, for example&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/addToCart?responseGroup=basic

/addToCart?responseGroup=full

/addToCart?responseGroup=summary
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The next problem to solve is, who will send this response group.&lt;/p&gt;

&lt;p&gt;In ideal situation, the client should not be worried about the response group, hardcoding these response group at the client side is maintainence nightmare and possible bugs. As the feature evolve, the client would have to worry about the appropriate response group to call all the time.&lt;/p&gt;

&lt;p&gt;That’s where &lt;a href=&quot;https://graphql.org/&quot;&gt;GraphQL&lt;/a&gt; comes into the picture. The client should just pass the fields it requires to a GraphQL service, call the appropriate &lt;a href=&quot;https://graphql.org/learn/queries/&quot;&gt;mutation&lt;/a&gt; and graphQL layer should have logic to determine the response and pass it to backend service.&lt;/p&gt;

&lt;p&gt;In nutshell, that’s how our architecture looks like:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/Blog/responsegroup.png&quot;&gt;&lt;img src=&quot;/images/Blog/responsegroup.png&quot; width=&quot;825px&quot; height=&quot;428px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Migrating from SQL to No-SQL without downtime</title>
   <link href="https://madhur.co.in/blog/2021/06/05/migrating-from-sql-nosql.html"/>
   <updated>2021-06-05T00:00:00+05:30</updated>
   <id>id:/blog/2021/06/05/migrating-from-sql-nosql</id>
   <content type="html">&lt;p&gt;Recently, we migrated a large MySQL database containing GB’s of data, millions of rows in few tables to CosmosDB.&lt;/p&gt;

&lt;p&gt;This was done without downtime. I want to lay down the approach we took in this post.&lt;/p&gt;

&lt;p&gt;Our intention was to setup both the database active - active and incrementally shift the traffic point to Service v1 (which was reading/writing to MySQL) towards 
Service v2 (which was reading/writing to CosmosDB)&lt;/p&gt;

&lt;p&gt;The first challenge was doing the initial data migration (called as bootstrap) and then set up a replication pipeline (called as change feed) so that any updates/inserts/deletes
happening on live MySQL DB are replicated to CosmosDB asynchronously.&lt;/p&gt;

&lt;p&gt;For this, we made use of Kafka.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The Bootstrap process consisted of reading the table from MySQL and dumping its content into a corresponding Kafka topic. That means, we had a corresponding Kafka topic for each
table within MySQL. There are products within market such as &lt;a href=&quot;https://www.continuent.com/products/tungsten-replicator&quot;&gt;this&lt;/a&gt; which can do this as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The change feed process consisted of having another set of parallel kafka topics which would contain the change feed, i.e. inserts / deletes / updates happening on the MySQL database.
This was tricky because setting it up requires modifying the service layer to publish this feeds to Kafka.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The point to note above is that bootstrap is a one time process and Change feed is an ongoing process. Also, bootstrap must be executed after the change feed process is up and running, so 
as to not miss any record.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Blog/migration.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now comes the consumer part,
The consumer application will connect to these kafka topics and store the data in Cosmos DB. We used bulk insertion in batches of 4000 records/batch to speed up the insertions.&lt;/p&gt;

&lt;p&gt;Here, the bootstrap kafka topics must be first completely consumed and written to target database before starting with ChangeFeed kafka topics.&lt;/p&gt;

&lt;p&gt;As the change feed kafka topics lag becomes closer to zero (it will never be zero, since data is always being written to change feed topics), the databases are considered active active.&lt;/p&gt;

&lt;p&gt;Few notes about Kafka:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Use Kafka partitions to scale up bootstrap and consumption process horizontally. The partition key can be primary key of the table. This is especially important in change feed 
topics where multiple records will exist for same row.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The consumer application must handle errors gracefully. If there is an error during the batch writes, the entire batch might fail.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Blue-green_deployment&quot;&gt;blue green&lt;/a&gt; deployment methodology&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In our case, it took almost 4 hrs to complete the migration including bootstrap and change feed process for around 3 million rows in couple of tables and fewer records in other tables.&lt;/p&gt;

</content>
 </entry>
 
 
</feed>